{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Naive Bayes</h2>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using approach from http://www.nltk.org/book/ch06.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import nltk\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comments_df = pd.read_csv(\"may2015equallyspaced.csv\")\n",
      "comments_df = comments_df.drop(['created_utc', 'subreddit_id', 'name', 'score_hidden', 'author_flair_css_class', \\\n",
      "                                'author_flair_text', 'removal_reason', 'gilded', 'archived', 'downs', 'score', \\\n",
      "                                'retrieved_on', 'distinguished', 'edited', 'controversiality', 'parent_id'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "included_subreddits = ['gameofthrones', 'worldnews', '2007scape', 'Showerthoughts']\n",
      "min_comments = min(len(comments_df[comments_df.subreddit == subreddit]) for subreddit in included_subreddits)\n",
      "print(min_comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "684\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Equal number of comments from each subreddit\n",
      "relevant_comments = []\n",
      "for subreddit in included_subreddits:\n",
      "    relevant_comments = relevant_comments + \\\n",
      "        [(comment.decode('utf-8').lower(), subreddit) for comment in \\\n",
      "                    list(comments_df[comments_df.subreddit == subreddit].body[0:min_comments])]\n",
      "relevant_comments[0::500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[(u'[deleted]', 'gameofthrones'),\n",
        " (u'stannis, a million miles away: it\\'s \"for whom do you fight?\"',\n",
        "  'gameofthrones'),\n",
        " (u'i have to represent us lol! ', 'worldnews'),\n",
        " (u'gotcha, thank you!', '2007scape'),\n",
        " (u'4 damage per hit if theres 5 hits.', '2007scape'),\n",
        " (u'big important tests are the worst. last minute cramming followed by me dumping out all the information once the test is over.',\n",
        "  'Showerthoughts')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordlist = []\n",
      "for comment in (x[0] for x in relevant_comments):\n",
      "    wordlist = wordlist + nltk.word_tokenize(comment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "worddist = nltk.FreqDist(wordlist)\n",
      "worddist.N()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "75732"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "common_words = [word for (word, count) in worddist.most_common(1000)]\n",
      "print(common_words[100:200], end=' ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'see', u'make', u'him', u'which', u'any', u'other', u'also', u'got', u'way', u'now', u'good', u\"'ve\", u'http', u'want', u'into', u'show', u'go', u'actually', u'though', u'should', u'still', u'say', u'those', u'too', u'sure', u'pretty', u'well', u'us', u'ca', u'going', u'very', u'its', u'most', u'something', u'right', u'never', u'said', u\"'d\", u'back', u'use', u'world', u'need', u'these', u'thing', u'bad', u'first', u'point', u'where', u'someone', u'many', u'am', u'lot', u'made', u'things', u'every', u'off', u'same', u'probably', u'shit', u\"'ll\", u'here', u'doing', u\"'\", u'%', u'-', u'\\\\n\\\\n', u'feel', u'since', u'new', u'maybe', u'man', u'day', u'\\\\n', u'take', u'around', u'season', u'while', u'game', u'better', u'always', u'https', u'look', u'thought', u'books', u'money', u'over', u'after', u'trying', u'read', u'amp', u'makes', u'guy', u'through', u'own', u'different', u'two', u'before', u'mean', u'old', u'without'] "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def comment_features(comment):\n",
      "    tokens = nltk.word_tokenize(comment)\n",
      "    features = {}\n",
      "    for word in common_words:\n",
      "        features['\\\"{}\\\" count'.format(word.encode('utf-8'))] = tokens.count(word)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(comment_features(comment), subreddit) for (comment, subreddit) in relevant_comments]\n",
      "len(featuresets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "2736"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(featuresets)\n",
      "train_set, test_set = featuresets[len(featuresets)/2:], featuresets[:len(featuresets)/2]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifier, test_set))\n",
      "classifier.show_most_informative_features(30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.432748538012\n",
        "Most Informative Features\n",
        "             \"she\" count = 1              gameof : worldn =     13.4 : 1.0\n",
        "            \"show\" count = 1              gameof : 2007sc =     12.6 : 1.0\n",
        "             \"her\" count = 1              gameof : worldn =     11.4 : 1.0\n",
        "              \"us\" count = 1              worldn : gameof =     10.6 : 1.0\n",
        "             \"got\" count = 1              2007sc : Shower =      9.4 : 1.0\n",
        "             \"was\" count = 2              gameof : Shower =      9.0 : 1.0\n",
        "             \"for\" count = 2              2007sc : Shower =      8.8 : 1.0\n",
        "             \"him\" count = 1              gameof : Shower =      8.3 : 1.0\n",
        "            \"said\" count = 1              gameof : Shower =      7.7 : 1.0\n",
        "            \"part\" count = 1              worldn : Shower =      7.2 : 1.0\n",
        "             \"jon\" count = 1              gameof : Shower =      7.0 : 1.0\n",
        "              \"oh\" count = 1              gameof : worldn =      6.8 : 1.0\n",
        "         \"against\" count = 1              worldn : 2007sc =      6.6 : 1.0\n",
        "           \"https\" count = 1              worldn : gameof =      6.5 : 1.0\n",
        "             \"you\" count = 3              worldn : gameof =      6.5 : 1.0\n",
        "             \"let\" count = 1              gameof : 2007sc =      6.5 : 1.0\n",
        "             \"his\" count = 1              gameof : Shower =      6.4 : 1.0\n",
        "           \"going\" count = 1              gameof : Shower =      6.2 : 1.0\n",
        "           \"point\" count = 1              2007sc : Shower =      6.2 : 1.0\n",
        "          \"happen\" count = 1              gameof : worldn =      6.2 : 1.0\n",
        "           \"world\" count = 1              worldn : 2007sc =      6.1 : 1.0\n",
        "          \"around\" count = 1              2007sc : worldn =      6.0 : 1.0\n",
        "          \"enough\" count = 1              worldn : 2007sc =      5.9 : 1.0\n",
        "            \"just\" count = 2              worldn : 2007sc =      5.9 : 1.0\n",
        "             \"the\" count = 6              worldn : 2007sc =      5.9 : 1.0\n",
        "          \"single\" count = 1              worldn : gameof =      5.8 : 1.0\n",
        "             \"war\" count = 1              worldn : gameof =      5.8 : 1.0\n",
        "             \"may\" count = 1              worldn : Shower =      5.8 : 1.0\n",
        "            \"book\" count = 1              gameof : 2007sc =      5.8 : 1.0\n",
        "            \"this\" count = 2              gameof : Shower =      5.7 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Random Forests</h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Conditional independence assumption of naive bayes is far from true here\n",
      "#Amount of data merits a more complex model like a random forest\n",
      "#Warning: big calculation\n",
      "included_subreddits = [sub for sub in comments_df.subreddit if len(comments_df[comments_df.subreddit == sub]) > 500]\n",
      "included_subreddits = list(set(included_subreddits))\n",
      "print(included_subreddits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Smite', 'buildapc', 'todayilearned', 'electronic_cigarette', 'Bitcoin', 'rupaulsdragrace', 'Music', 'TrollXChromosomes', 'tifu', 'ffxiv', 'relationships', 'GlobalOffensive', 'SquaredCircle', 'Showerthoughts', '2007scape', 'hockey', 'WTF', 'whowouldwin', 'explainlikeimfive', 'Fireteams', 'fatpeoplehate', 'hiphopheads', 'gifs', 'trees', 'smashbros', 'newsokur', 'asoiaf', 'AskMen', 'gonewild', 'soccer', 'nfl', 'movies', 'AdviceAnimals', 'SubredditDrama', 'witcher', 'MMA', 'mildlyinteresting', 'unitedkingdom', 'videos', 'AskWomen', 'india', 'KotakuInAction', 'Christianity', 'Fitness', 'TwoXChromosomes', 'politics', 'nba', 'technology', 'Android', 'PS4', 'funny', 'gameofthrones', 'heroesofthestorm', 'thebutton', 'wow', 'DestinyTheGame', 'IAmA', 'AskReddit', 'amiibo', 'leagueoflegends', 'worldnews', 'pcmasterrace', 'GlobalOffensiveTrade', 'magicTCG', 'DotA2', 'europe', 'gaming', 'aww', 'OkCupid', 'Games', 'hearthstone', 'news', 'CasualConversation', 'csgobetting', 'CFB', 'personalfinance', 'pics', 'atheism', 'TumblrInAction', 'anime', 'baseball']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#included_subreddits = set(included_subreddits)\n",
      "#Shortcut:\n",
      "included_subreddits = ['Smite', 'buildapc', 'unitedkingdom', 'videos', 'AskWomen', 'todayilearned', 'asoiaf', 'india', 'mildlyinteresting', \\\n",
      "                       'rupaulsdragrace', 'Christianity', 'Music', 'TwoXChromosomes', 'TrollXChromosomes', 'politics', 'nba', 'ffxiv', 'tifu', \\\n",
      "                       'PS4', 'hockey', 'relationships', 'funny', 'gameofthrones', 'technology', 'heroesofthestorm', 'thebutton', 'gonewild', \\\n",
      "                       'wow', 'DestinyTheGame', 'IAmA', 'AdviceAnimals', 'Showerthoughts', 'amiibo', 'SubredditDrama', '2007scape', 'leagueoflegends', \\\n",
      "                       'WTF', 'worldnews', 'whowouldwin', 'pcmasterrace', 'explainlikeimfive', 'GlobalOffensiveTrade', 'magicTCG', 'KotakuInAction', \\\n",
      "                       'DotA2', 'europe', 'gaming', 'electronic_cigarette', 'hiphopheads', 'newsokur', 'GlobalOffensive', 'aww', 'fatpeoplehate', \\\n",
      "                       'smashbros', 'Android', 'Games', 'trees', 'Fitness', 'hearthstone', 'news', 'AskMen', 'Bitcoin', 'OkCupid', 'gifs', 'soccer', \\\n",
      "                       'MMA', 'csgobetting', 'nfl', 'CFB', 'personalfinance', 'pics', 'atheism', 'movies', 'AskReddit', 'TumblrInAction', 'anime', \\\n",
      "                       'witcher', 'baseball', 'Fireteams', 'CasualConversation', 'SquaredCircle']\n",
      "print(len(included_subreddits))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "81\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NONequal number of comments from each subreddit\n",
      "relevant_comments = []\n",
      "for subreddit in included_subreddits:\n",
      "    relevant_comments = relevant_comments + \\\n",
      "        [(str(comment).decode('utf-8').lower(), subreddit) for comment in \\\n",
      "                    list(comments_df[comments_df.subreddit == subreddit].body)]\n",
      "relevant_comments[0::10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[(u\"however, what you have to remember about that is that these are professional teams who know what the enemy team is good at. in regular league there's no way to know. therefore, the same people get banned over and over again to be safe. \",\n",
        "  'Smite'),\n",
        " (u'this is not a case where \"whoops, i\\'m sorry\" is sufficient. nor is it too friendly people who had a misunderstanding. it\\'s a person who was detained for half a day by the incompetence of a corporation\\'s employees and absolutely perplexing incompetence of some law enforcement (who should have instantly recognized the bills as legit).\\\\n\\\\ni have never sued anybody but i have been detained for extended period under completely unfounded suspicions. to put it mildly, it sucks. this person deserves some money for their inconvenience and their suffering. taco bell also deserves to pay for hiring moronic employees. ',\n",
        "  'mildlyinteresting'),\n",
        " (u\"[jagr has a disagreement in today's game at worlds](http://imgur.com/neppgnc)\",\n",
        "  'hockey'),\n",
        " (u'i stil think you have a great ass! i would love to be behind you. can you imagine my hands on your ass?',\n",
        "  'gonewild'),\n",
        " (u'the most nicer garen in the league. ok.', 'leagueoflegends'),\n",
        " (u'[deleted]', 'worldnews'),\n",
        " (u\"don't forget us nebraskans!\", 'gaming'),\n",
        " (u'[deleted]', 'Games'),\n",
        " (u'eventually this has to take a toll on players wanting to join real madrid.  ',\n",
        "  'soccer'),\n",
        " (u'[deleted]', 'pics'),\n",
        " (u\"maybe they're more clean and actually wash?\", 'AskReddit'),\n",
        " (u\"yeah, your recollection of the event is bullshit.\\\\nthere was no talk of water landings.\\\\nit wasn't a dire emergency.\\\\n\\\\nthe flight crew requested a diversion for a technical issue that they weren't comfortable (rightfully so) continuing a transatlantic flight with.\\\\n\\\\nthey were receiving messages in the cockpit about a fuel filter having an issue so requested to divert to one of their diversion airports (within their 120min edto),  it's fairly standard.\\\\nthey later declared an emergency to the tower only due to the fact they would be making a landing with a lot of residual fuel, which means a fairly heavy landing.\\\\n\\\\nhowever, once they started their final approach, after a 90min journey to their diversion airport, they were believed to be within good limits for landing, and they were not in an emergency scenario.\\\\n\\\\n\\\\nyour imagination has run wild with you, your interpretation did not occur whatsoever.\\\\n\\\\nthe pilots were well aware of the fault they were experiencing.\\\\n\\\\nyou were not in an 'almost plane crash' you numpty.  you experience a diversion, that is all.  diversions happen more frequently than we would like due to all sorts of situations, closure of runways at the airport we intend to land at, unfavorable weather conditions at the destination airport, unfavorable weather conditions in flight, mechanical or technical issues, passenger unruliness, passenger medical emergencies, incidents at destination airport etc etc\",\n",
        "  'AskReddit'),\n",
        " (u\"because i'm just sharing my thoughts on a thing. if i'm wrong i'm wrong, i wasn't aware that i needed a phd in any subject i wanted to share an errant thought about.\",\n",
        "  'SquaredCircle')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Takes a couple hours...\n",
      "wordlist = []\n",
      "for comment in (x[0] for x in relevant_comments):\n",
      "    wordlist = wordlist + nltk.word_tokenize(comment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Write wordlist to file\n",
      "#with open('dswordlist.pickle', 'wb') as f:\n",
      "#    pickle.dump(wordlist, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read wordlist from file\n",
      "with open('dswordlist.pickle', 'rb') as f:\n",
      "    wordlist = pickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "worddist = nltk.FreqDist(wordlist)\n",
      "worddist.N()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "3861163"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#high tfidf might be better than common words (or a mixture of the two)\n",
      "#Probably could do significantly better if more words included (if machine can handle it)\n",
      "common_words = [word for (word, count) in worddist.most_common(1000)]\n",
      "print(common_words[900:1000], end=' ')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'blue', u'losing', u'several', u'max', u'relevant', u'depends', u'available', u'cat', u'putting', u'eventually', u'none', u'mad', u'slow', u'karma', u'sucks', u'date', u'female', u'sit', u's', u'attention', u'share', u'carry', u'moving', u'respect', u'double', u'kept', u'places', u'recently', u'buying', u'paying', u'drink', u'speak', u'added', u'popular', u'imo', u'inside', u'12', u'male', u'considering', u'defense', u'religion', u'episode', u'solid', u'killing', u'cheap', u'building', u'local', u'piece', u'fall', u'context', u'particular', u'nope', u'p', u'media', u'standard', u'blood', u'safe', u'to=', u'gotten', u'liked', u'\\\\n\\\\nedit', u'rights', u'terms', u'knowing', u'view', u'willing', u'difficult', u'\\\\n\\\\nyou', u'box', u'disagree', u'sign', u'2fr', u'range', u'heart', u'companies', u'died', u'listen', u'related', u'meet', u'claim', u'users', u'plays', u'test', u'goal', u'honest', u'avoid', u'dumb', u'father', u'plenty', u'effect', u'service', u'allow', u'spent', u'broken', u'complete', u'bitch', u'bigger', u'enemy', u'biggest', u'field'] "
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_list(comment):\n",
      "    tokens = nltk.word_tokenize(comment)\n",
      "    return [tokens.count(word) for word in common_words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(relevant_comments)\n",
      "print(relevant_comments[0:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'coop_096', 'Fireteams'), (u'whatever, you can never get that last block anyways', 'gaming'), (u\"if your looking to upgrade your keyboard but don't want to pay for the razer pricetag you should stop by /r/mechanicalkeyboards there are plenty of people that can help you find the right keyboard for you and your wallet.\", 'pcmasterrace'), (u'what does that mean', 'leagueoflegends'), (u'snowden?  is that you?', 'funny'), (u'another one bites the dust', 'AskReddit'), (u'meant na bro. my bad.', 'GlobalOffensive'), (u'the real question is does reek know the difference between less and fewer. ', 'asoiaf'), (u'and the third saturday in october...\\\\n\\\\n^^wait ^^nevermind', 'CFB'), (u'isnt it the other way around?', 'Showerthoughts')]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Only include comments containing at least min_length words\n",
      "min_length = 100\n",
      "#Only including 1 in 5 of the relevant_comments\n",
      "X = np.array([feature_list(comment[0]) for comment in relevant_comments[::5] if len(comment[0]) > min_length])\n",
      "y = np.array([comment[1] for comment in relevant_comments[::5] if len(comment[0]) > min_length])\n",
      "print(X[0:10])\n",
      "print(y[0:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 7  3 14 ...,  0  0  0]\n",
        " [ 0  0  0 ...,  0  0  0]\n",
        " [ 2  4  6 ...,  0  0  0]\n",
        " ..., \n",
        " [ 1  3  2 ...,  0  0  0]\n",
        " [ 2  1  2 ...,  0  0  0]\n",
        " [ 1  1  0 ...,  0  0  0]]\n",
        "['AskReddit' 'tifu' 'SquaredCircle' 'GlobalOffensive' 'TwoXChromosomes'\n",
        " 'nfl' 'worldnews' 'nba' 'wow' 'news']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split = int(np.floor(X.shape[0]*0.75))\n",
      "X_train, X_test = X[:split], X[split:]\n",
      "y_train, y_test = y[:split], y[split:]\n",
      "print(X_test.shape, y_test.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2441, 1000) (2441,)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "# scoreMethod='accuracy'\n",
      "# cvCount = 10\n",
      "# jobs = 2\n",
      "# verbosity = 1\n",
      "# clf = RandomForestClassifier()\n",
      "# nrange = range(2,30)\n",
      "# mrange = range(2,30)\n",
      "# grid = GridSearchCV(clf, param_grid={'criterion': ['gini', 'entropy'], 'min_samples_split':mrange, \\\n",
      "#         'bootstrap': [False, True], 'n_estimators':nrange},\n",
      "# scoring=scoreMethod, cv=cvCount, n_jobs=jobs, verbose=verbosity)\n",
      "\n",
      "#min_samples_split best determined to be 2 in previous grid searches. same with bootstrap\n",
      "scoreMethod='accuracy'\n",
      "cvCount = 3\n",
      "jobs = 1\n",
      "verbosity = 1\n",
      "clf = RandomForestClassifier()\n",
      "nrange = range(30,500, 5)\n",
      "mrange = range(2,20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Big search - takes a very long time\n",
      "grid = GridSearchCV(clf, param_grid={'criterion': ['gin', 'entropy'], 'min_samples_split':mrange, \\\n",
      "        'bootstrap': [True, False], 'n_estimators':nrange},\n",
      "scoring=scoreMethod, cv=cvCount, n_jobs=jobs, verbose=verbosity)\n",
      "\n",
      "start_time = time.time()\n",
      "\n",
      "grid.fit(X_train,y_train)\n",
      "\n",
      "print(\"Took \" + str(time.time() - start_time) + \" seconds\")\n",
      "\n",
      "print(grid.best_params_)\n",
      "print(grid.best_score_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Small search\n",
      "nrange = range(100, 500, 50)\n",
      "grid = GridSearchCV(clf, param_grid={'criterion': ['entropy'], 'min_samples_split':[2], \\\n",
      "        'bootstrap': [False], 'n_estimators':nrange},\n",
      "scoring=scoreMethod, cv=cvCount, n_jobs=jobs, verbose=verbosity)\n",
      "\n",
      "start_time = time.time()\n",
      "\n",
      "grid.fit(X_train,y_train)\n",
      "\n",
      "print(\"Took \" + str(time.time() - start_time) + \" seconds\")\n",
      "\n",
      "print(grid.best_params_)\n",
      "print(grid.best_score_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:   19.6s\n",
        "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 20.4min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
        "Took 1286.49860501 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'min_samples_split': 2, 'n_estimators': 200, 'bootstrap': False, 'criterion': 'entropy'}\n",
        "0.181075090053"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = RandomForestClassifier(n_estimators=200, bootstrap=False, criterion='entropy')\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "RandomForestClassifier(bootstrap=False, compute_importances=None,\n",
        "            criterion='entropy', max_depth=None, max_features='auto',\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=200, n_jobs=1, oob_score=False, random_state=None,\n",
        "            verbose=0)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.18926669397787793"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict(feature_list(u\"however, what you have to remember about that is that these are professional teams who know what the enemy \\\n",
      "            team is good at. in regular league there's no way to know. therefore, the same people get banned over and over \\\n",
      "            again to be safe. \"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array(['leagueoflegends'], \n",
        "      dtype='|S20')"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}